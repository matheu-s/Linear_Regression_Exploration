{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4832081,"sourceType":"datasetVersion","datasetId":2799910}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/matheus133/linear-regression-library-and-coded-from-scratch?scriptVersionId=200371384\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Exploring Linear Regression: A Comparison of Scikit-learn LR model and our model coded from scratch!\n\n## Introduction\n\nIn this notebook, we will explore **linear regression**, a fundamental algorithm in machine learning used to model the relationship between a dependent variable and one or more independent variables.\n\nSpecifically, we will:\n\n- Utilize **Scikit-learnâ€™s LinearRegression** implementation to quickly create and train a linear model.\n- Code our own version of **gradient descent** from scratch to understand the underlying mathematics and mechanics of linear regression.\n\nWe will apply both methods to a **univariate linear regression problem**, where:\n\n- **X** represents the **years of experience** of employees.\n- **Y** represents the corresponding **salary amounts**.\n\n### Objective\n\nThe primary goal is to compare the results of Scikit-learnâ€™s built-in linear regression with our manually coded gradient descent. By doing this, we can gain a better understanding of how the algorithms work from scratch.\n\n### What you'll learn:\n\n- How to implement and use **Scikit-learn's LinearRegression** to fit a simple linear model.\n- How to **code gradient descent from scratch**, step by step, to solve the same problem.\n- **Compare** the results of both approaches.\n\nLet's dive into the world of **linear regression** and discover how the two approaches fare when applied to the years of experience vs. salary dataset!\n\nInspired by Andrew Ng.\n","metadata":{}},{"cell_type":"markdown","source":"## Initial data exploration and fixes","metadata":{}},{"cell_type":"code","source":"# Importing libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression","metadata":{"ExecuteTime":{"end_time":"2024-10-10T18:44:43.56615Z","start_time":"2024-10-10T18:44:43.435187Z"},"execution":{"iopub.status.busy":"2024-10-10T19:03:21.801014Z","iopub.execute_input":"2024-10-10T19:03:21.801525Z","iopub.status.idle":"2024-10-10T19:03:21.807782Z","shell.execute_reply.started":"2024-10-10T19:03:21.801447Z","shell.execute_reply":"2024-10-10T19:03:21.806637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load the dataset into a pandas DataFrame\ndf = pd.read_csv('/kaggle/input/salary-dataset-simple-linear-regression/Salary_dataset.csv')\n\n\n# Display the first few rows of the DataFrame\ndf.head()","metadata":{"ExecuteTime":{"end_time":"2024-10-10T18:44:43.69088Z","start_time":"2024-10-10T18:44:43.593721Z"},"execution":{"iopub.status.busy":"2024-10-10T19:03:21.81061Z","iopub.execute_input":"2024-10-10T19:03:21.811139Z","iopub.status.idle":"2024-10-10T19:03:21.834245Z","shell.execute_reply.started":"2024-10-10T19:03:21.811094Z","shell.execute_reply":"2024-10-10T19:03:21.833052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Exclude first column (id)\ndf = df.iloc[:, 1:]\ndf.head()","metadata":{"ExecuteTime":{"end_time":"2024-10-10T18:44:43.766473Z","start_time":"2024-10-10T18:44:43.760317Z"},"execution":{"iopub.status.busy":"2024-10-10T19:03:21.835915Z","iopub.execute_input":"2024-10-10T19:03:21.836333Z","iopub.status.idle":"2024-10-10T19:03:21.851138Z","shell.execute_reply.started":"2024-10-10T19:03:21.836285Z","shell.execute_reply":"2024-10-10T19:03:21.849687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation matrix\ndf.corr()","metadata":{"ExecuteTime":{"end_time":"2024-10-10T18:44:43.839866Z","start_time":"2024-10-10T18:44:43.832859Z"},"execution":{"iopub.status.busy":"2024-10-10T19:03:21.852817Z","iopub.execute_input":"2024-10-10T19:03:21.853714Z","iopub.status.idle":"2024-10-10T19:03:21.869311Z","shell.execute_reply.started":"2024-10-10T19:03:21.853656Z","shell.execute_reply":"2024-10-10T19:03:21.867858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"years_experience = df['YearsExperience'].values.reshape(-1, 1) # convert the data into a 2D array\nsalaries = df['Salary'].values  ","metadata":{"ExecuteTime":{"end_time":"2024-10-10T18:44:43.923165Z","start_time":"2024-10-10T18:44:43.917941Z"},"execution":{"iopub.status.busy":"2024-10-10T19:03:21.873146Z","iopub.execute_input":"2024-10-10T19:03:21.873587Z","iopub.status.idle":"2024-10-10T19:03:21.882292Z","shell.execute_reply.started":"2024-10-10T19:03:21.873544Z","shell.execute_reply":"2024-10-10T19:03:21.880834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear regression from Sklearn","metadata":{}},{"cell_type":"code","source":"# Splitting the data to train and to validate (test) the model later\nX_train, X_test, y_train, y_test = train_test_split(years_experience, salaries, test_size=0.2, random_state=0)","metadata":{"ExecuteTime":{"end_time":"2024-10-10T18:44:44.007354Z","start_time":"2024-10-10T18:44:43.988462Z"},"execution":{"iopub.status.busy":"2024-10-10T19:03:21.883996Z","iopub.execute_input":"2024-10-10T19:03:21.884662Z","iopub.status.idle":"2024-10-10T19:03:21.896073Z","shell.execute_reply.started":"2024-10-10T19:03:21.884605Z","shell.execute_reply":"2024-10-10T19:03:21.894608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using the LinearRegression model from sklearn library\nsimple_linear_regression = LinearRegression()\nsimple_linear_regression.fit(X_train, y_train)","metadata":{"ExecuteTime":{"end_time":"2024-10-10T18:44:44.079766Z","start_time":"2024-10-10T18:44:44.047632Z"},"execution":{"iopub.status.busy":"2024-10-10T19:03:21.897583Z","iopub.execute_input":"2024-10-10T19:03:21.898Z","iopub.status.idle":"2024-10-10T19:03:21.915297Z","shell.execute_reply.started":"2024-10-10T19:03:21.897957Z","shell.execute_reply":"2024-10-10T19:03:21.913839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Just taking a look at the found W, for curiosity...\nw = simple_linear_regression.coef_\n# Just taking a look at the found B, for curiosity too...\nn = simple_linear_regression.intercept_\n\nprint(\"The w parameter is:\", w)\nprint(\"The bias is:\", n)","metadata":{"ExecuteTime":{"end_time":"2024-10-10T18:44:44.147609Z","start_time":"2024-10-10T18:44:44.143321Z"},"execution":{"iopub.status.busy":"2024-10-10T19:03:21.916927Z","iopub.execute_input":"2024-10-10T19:03:21.917488Z","iopub.status.idle":"2024-10-10T19:03:21.927472Z","shell.execute_reply.started":"2024-10-10T19:03:21.917446Z","shell.execute_reply":"2024-10-10T19:03:21.926123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicting the result for X never seen before \ny_pred = simple_linear_regression.predict(X_test)\n\n# Prediction on trained data\ny_pred_train = simple_linear_regression.predict(X_train)","metadata":{"ExecuteTime":{"end_time":"2024-10-10T18:44:44.168592Z","start_time":"2024-10-10T18:44:44.165777Z"},"execution":{"iopub.status.busy":"2024-10-10T19:03:21.929183Z","iopub.execute_input":"2024-10-10T19:03:21.929731Z","iopub.status.idle":"2024-10-10T19:03:21.947406Z","shell.execute_reply.started":"2024-10-10T19:03:21.929674Z","shell.execute_reply":"2024-10-10T19:03:21.94589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting graph\nplt.scatter(X_train, y_train, color = 'r', label = 'Actual Salary')\nplt.plot(X_train, y_pred_train, color = \"b\", label = 'Predicted Salary')\nplt.plot(X_train,simple_linear_regression.predict(X_train), color = 'b')\nplt.title('Salary VS Experience (Training set) ')\nplt.xlabel('Years of Experience')\nplt.ylabel('Salary')\nplt.legend()\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2024-10-10T18:44:44.465243Z","start_time":"2024-10-10T18:44:44.190092Z"},"execution":{"iopub.status.busy":"2024-10-10T19:03:21.949164Z","iopub.execute_input":"2024-10-10T19:03:21.949616Z","iopub.status.idle":"2024-10-10T19:03:22.288077Z","shell.execute_reply.started":"2024-10-10T19:03:21.949573Z","shell.execute_reply":"2024-10-10T19:03:22.286873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Coding a linear regression model from scratch\n### But yeah... that was pretty easy, right?\n\nNo real code needed, no functions, no math! *hehehe* ðŸ˜„\n\nNow, let's make it a bit more **interesting**. \n\nInstead of using the `Sklearn` `LinearRegression` model, why don't we create our own model from scratch? ðŸš€\n\n---","metadata":{}},{"cell_type":"markdown","source":"#### Model function equation\nSo, first, let's code our fundamental function piece, the code to represent the model equation:\n$$\n\\color{red}{f_{wb}(x) = wx+b}\n$$\nThis function is the pilar of our linear regression, as it defines `y` predictions according to `x` and the parameters `w` and `b`. Considering we know the values `w` and `b`, we could predict the output... but now there is a long way to actually find these values.","metadata":{}},{"cell_type":"code","source":"def f_wb(x, w_param, b_param):\n    return x * w_param + b_param","metadata":{"ExecuteTime":{"end_time":"2024-10-10T18:44:44.50573Z","start_time":"2024-10-10T18:44:44.491793Z"},"execution":{"iopub.status.busy":"2024-10-10T19:03:22.289591Z","iopub.execute_input":"2024-10-10T19:03:22.290024Z","iopub.status.idle":"2024-10-10T19:03:22.297114Z","shell.execute_reply.started":"2024-10-10T19:03:22.289982Z","shell.execute_reply":"2024-10-10T19:03:22.295558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Cost function\nNow, let's define the cost function, this will function is represented by the mathematical equation: \n$$\n\\color{orange}{J(w, b)} = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\color{red}{f_{wb}(x^{(i)})} - y^{(i)} \\right)^2\n$$\nIt basically defines how much error was aggregated when iterating all the training X values, for certain  set parameters `w` and `b`. Where error is the difference between the predicted and the actual `y`.","metadata":{}},{"cell_type":"code","source":"def cost_j(w, b, x_train, y_train):\n    \"\"\"\n    Computes the cost function for linear regression.\n    \n    Args:\n        x (ndarray): Shape (m,) Input to the model (Population of cities) \n        y (ndarray): Shape (m,) Label (Actual profits for the cities)\n        w, b (scalar): Parameters of the model\n    \n    Returns\n        total_cost (float): The cost of using w,b as the parameters for linear regression\n               to fit the data points in x and y\n    \"\"\"\n    m = len(x_train)\n    total_cost = 0\n    for i in range(m):\n        predicted_value = f_wb(X_train[i], w, b)\n        target_value = y_train[i]\n        total_cost += (predicted_value - target_value)**2\n    \n    return total_cost/(2*m)","metadata":{"ExecuteTime":{"end_time":"2024-10-10T18:44:44.531729Z","start_time":"2024-10-10T18:44:44.527158Z"},"execution":{"iopub.status.busy":"2024-10-10T19:03:22.299047Z","iopub.execute_input":"2024-10-10T19:03:22.299486Z","iopub.status.idle":"2024-10-10T19:03:22.309044Z","shell.execute_reply.started":"2024-10-10T19:03:22.29944Z","shell.execute_reply":"2024-10-10T19:03:22.30788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Gradient descent\nCommon linear regression algorithm that has its purpose on minimizing errors between the predicted and actual values. Its goal is to converge the parameters `w` and `b` into the minimal cost possible, by doing that, it comes up with the best and correct parameters that will be used to predict `y`.\n\nHere is the mathematical model:\n$$\n\\text{repeat until convergence \\{} \\\\\n\\quad w = w - \\alpha \\color{blue}{\\frac{\\partial}{\\partial w} J(w, b)} \\\\\n\\quad b = b - \\alpha \\color{purple}{\\frac{\\partial}{\\partial b} J(w, b)} \\\\\n\\text{\\}}\n$$\n\nLet's break down the derivation:\n\n$$\n\\color{blue}{\\frac{\\partial J(w, b)}{\\partial w_j}} = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left( f_{w,b}(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}\n$$\n\n$$\n\\color{purple}{\\frac{\\partial J(w, b)}{\\partial b}} = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left( f_{w,b}(x^{(i)}) - y^{(i)} \\right)\n$$","metadata":{}},{"cell_type":"code","source":"def compute_gradient(X, y, w, b):\n    \"\"\"\n    Computes the gradient for linear regression \n    Args:\n      X (ndarray (m,n)): Data, m examples with n features\n      y (ndarray (m,)) : target values\n      w (ndarray (n,)) : model parameters  \n      b (scalar)       : model parameter\n\n    Returns:\n      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n    \"\"\"\n    m = len(X)\n    dj_dw = 0 \n    dj_db = 0              \n    for i in range(m):\n        predict = f_wb(X[i], w, b)\n        err = predict - y[i]\n        dj_dw += err*X[i]\n        dj_db += err\n\n    dj_dw = dj_dw / m\n    dj_db = dj_db / m\n    \n    return dj_dw, dj_db","metadata":{"ExecuteTime":{"end_time":"2024-10-10T18:44:44.551239Z","start_time":"2024-10-10T18:44:44.548794Z"},"execution":{"iopub.status.busy":"2024-10-10T19:03:22.310882Z","iopub.execute_input":"2024-10-10T19:03:22.311335Z","iopub.status.idle":"2024-10-10T19:03:22.322934Z","shell.execute_reply.started":"2024-10-10T19:03:22.31129Z","shell.execute_reply":"2024-10-10T19:03:22.321716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And now the full algorithm, updating the parameters until convergence:","metadata":{}},{"cell_type":"code","source":"def gradient_descent(X, y, initial_w, initial_b, gradient_f, alpha, num_iters):\n    \"\"\"\n    Performs batch gradient descent to learn w and b. Updates w and b by taking \n    num_iters gradient steps with learning rate alpha\n    \n    Args:\n      X (ndarray (m,n))   : Data, m examples with n features\n      y (ndarray (m,))    : target values\n      initial_w (ndarray (n,)) : initial model parameters  \n      initial_b (scalar)       : initial model parameter\n      gradient_f   : function to compute the gradient\n      alpha (float)       : Learning rate\n      num_iters (int)     : number of iterations to run gradient descent\n      \n    Returns:\n      w (ndarray (n,)) : Updated values of parameters \n      b (scalar)       : Updated value of parameter \n      \"\"\"\n    w = initial_w\n    b = initial_b\n    for i in range(num_iters):\n        dj_dw,dj_db = gradient_f(X, y, w, b)   ## compute_gradient from upper in this case\n        w = w - (alpha * dj_dw)\n        b = b - (alpha * dj_db)\n    \n    return w, b","metadata":{"ExecuteTime":{"end_time":"2024-10-10T18:44:44.578988Z","start_time":"2024-10-10T18:44:44.573962Z"},"execution":{"iopub.status.busy":"2024-10-10T19:03:22.328163Z","iopub.execute_input":"2024-10-10T19:03:22.329135Z","iopub.status.idle":"2024-10-10T19:03:22.341245Z","shell.execute_reply.started":"2024-10-10T19:03:22.329084Z","shell.execute_reply":"2024-10-10T19:03:22.339957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting new chunks of data to train and to validate (test) the model later\nX_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(years_experience, salaries, test_size=0.2, random_state=0)\n\ntmp_alpha = 0.05 # Defining some alpha parameter\niterations = 1000\n\nw, b = gradient_descent(X_train_2, y_train_2, 1, 1, compute_gradient, tmp_alpha, iterations)\n\nprint(\"The w parameter is:\", w)\nprint(\"The bias is:\", n)","metadata":{"ExecuteTime":{"end_time":"2024-10-10T18:44:44.680996Z","start_time":"2024-10-10T18:44:44.59017Z"},"execution":{"iopub.status.busy":"2024-10-10T19:03:22.34264Z","iopub.execute_input":"2024-10-10T19:03:22.343053Z","iopub.status.idle":"2024-10-10T19:03:22.618209Z","shell.execute_reply.started":"2024-10-10T19:03:22.343013Z","shell.execute_reply":"2024-10-10T19:03:22.616728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\nfor i in range(len(X_train)):\n    # Here we predict the values based on the w and b found.\n    predictions.append(f_wb(X_train_2[i], w, b))","metadata":{"ExecuteTime":{"end_time":"2024-10-10T18:44:44.700435Z","start_time":"2024-10-10T18:44:44.697836Z"},"execution":{"iopub.status.busy":"2024-10-10T19:03:22.620249Z","iopub.execute_input":"2024-10-10T19:03:22.620756Z","iopub.status.idle":"2024-10-10T19:03:22.630687Z","shell.execute_reply.started":"2024-10-10T19:03:22.620697Z","shell.execute_reply":"2024-10-10T19:03:22.629397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(X_train_2, y_train_2, color = 'r', label = 'Actual Salary')\nplt.plot(X_train_2, predictions, color = \"b\", label = 'Predicted Salary')\nplt.title('Salary VS Experience (Training set) Coded from scratch ')\nplt.xlabel('Years of Experience')\nplt.ylabel('Salary')\nplt.legend()\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2024-10-10T18:44:44.772704Z","start_time":"2024-10-10T18:44:44.712542Z"},"execution":{"iopub.status.busy":"2024-10-10T19:03:22.632845Z","iopub.execute_input":"2024-10-10T19:03:22.633408Z","iopub.status.idle":"2024-10-10T19:03:22.927029Z","shell.execute_reply.started":"2024-10-10T19:03:22.633349Z","shell.execute_reply":"2024-10-10T19:03:22.925642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here it's possible to see how similar the results are to the `Sklearn` model. Even the found `w` and `b` fall really close to each other.","metadata":{}}]}